<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Aravinda VK</title><link href="http://aravindavk.in/" rel="alternate"></link><link href="http://aravindavk.in/feeds/tag-glusterfsblog.atom.xml" rel="self"></link><id>http://aravindavk.in/</id><updated>2016-03-14T00:00:00+05:30</updated><entry><title>Qcow2 snapshots and Gluster Geo-replication</title><link href="http://aravindavk.in/blog/qcow2-snapshots-and-gluster-georeplication/" rel="alternate"></link><updated>2016-03-14T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2016-03-14:blog/qcow2-snapshots-and-gluster-georeplication/</id><summary type="html">&lt;p&gt;Gluster introduced sharding feature to store large files(which can
grow beyond a single brick) or to support running Virtual machines in
Gluster Volumes. Read more about sharding &lt;a class="reference external" href="http://blog.gluster.org/2015/12/introducing-shard-translator/"&gt;here&lt;/a&gt; and
&lt;a class="reference external" href="http://blog.gluster.org/2015/12/sharding-what-next-2/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="how-to-backup-vm-images"&gt;
&lt;h2&gt;How to backup VM images?&lt;/h2&gt;
&lt;p&gt;Backing up VM images is not easy, rsync will consume more CPU to
calculate the checksum to sync only incremental changes.&lt;/p&gt;
&lt;p&gt;Geo-replication is aware of Gluster Sharding feature and taking the
advantage of syncing small sharded files instead of big qcow2 image
files. But is the data consistent? In this blog we will understand how
to backup VM images to DR site consistently.&lt;/p&gt;
&lt;p&gt;Read &lt;a class="reference external" href="http://hrkscribbles.blogspot.in/2016/02/gluster-geo-replication-with-sharding.html"&gt;here&lt;/a&gt; to know more about Geo-replication support for sharding.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="setup"&gt;
&lt;h2&gt;Setup:&lt;/h2&gt;
&lt;p&gt;VMs hosted in Gluster Volume(Master Volume) and Geo-replicated to
another Gluster Volume(Slave/Backup Volume). Sharding enabled in both
the Gluster Volumes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-internal-qcow2-snapshot"&gt;
&lt;h2&gt;Using Internal Qcow2 snapshot:&lt;/h2&gt;
&lt;p&gt;Before starting Geo-replication every day, take qcow2 snapshot of all
the disks in Master Volume. Geo-rep will sync the data including the
created snapshots.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;virsh snapshot-create-as --domain &amp;lt;DOMAIN&amp;gt; &amp;lt;SNAP_NAME&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;virsh snapshot-create-as --domain fedora22 sn1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But while taking internal snapshot, guest is &lt;strong&gt;paused&lt;/strong&gt; :(&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# virsh list
 Id    Name                           State
----------------------------------------------------
 3     fedora22                       paused
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If Guest has more RAM and actively modifying state, then more
time to take Snapshot.&lt;/p&gt;
&lt;p&gt;Run Geo-replication using the scheduler script, which will
Set the checkpoint and automatically stops Geo-replication once
checkpoint is reached.(This utility will be available with
&lt;cite&gt;glusterfs-3.7.9&lt;/cite&gt; release.)&lt;/p&gt;
&lt;p&gt;Run &lt;tt class="docutils literal"&gt;/usr/share/glusterfs/scripts/schedule_georep.py &lt;span class="pre"&gt;--help&lt;/span&gt;&lt;/tt&gt; for more
details about the script.(&lt;tt class="docutils literal"&gt;/usr/local/share/&lt;/tt&gt; in case of source install)&lt;/p&gt;
&lt;p&gt;Psudeo code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pool_dir=&amp;lt;PATH_OF_MASTER_VOL_MOUNT&amp;gt;
images=$(ls ${pool_dir})
For each images, take qcow2 snapshot
Run schedule_georep script
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the scheduler script completes, check the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;qemu-img&lt;/span&gt; info&lt;/tt&gt; in Slave
and confirm that Geo-rep synced everything from master Volume
including Snapshots created.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;qemu-img info /mnt/gv2/f22.qcow2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Example Output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;image: /mnt/gv2/f22.qcow2
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 2.8G
cluster_size: 65536
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
2         sn2                    693M 2016-02-23 18:40:10   01:37:34.881
3         sn3                    693M 2016-02-23 18:47:06   01:44:15.950
Format specific information:
    compat: 1.1
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="using-external-qcow2-snapshot"&gt;
&lt;h2&gt;Using External Qcow2 snapshot&lt;/h2&gt;
&lt;p&gt;Once we take external snapshot, qcow2 image will become read only base
image and snapshot file will become overlay(Read more about backing
chain and overlay &lt;a class="reference external" href="https://kashyapc.fedorapeople.org/virt/lc-2012/snapshots-handout.html"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;New changes will be recorded in the overlay instead of base image,
Since base image is frozen Geo-rep will sync the consistent image to
Slave. Start Geo-replication and wait for scheduler script to end.&lt;/p&gt;
&lt;p&gt;When multiple external snapshots taken, it is very difficult to
maintain the backing chain and reverting to a snapshot is not easy
when external snapshot is used. Once Geo-rep Scheduler script is
complete, blockcommit the image in Master side to prevent growing
backing chain.&lt;/p&gt;
&lt;p&gt;Delete the external snapshot once the blockcommit returns success.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;pool_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;PATH_OF_MASTER_VOL_MOUNT&amp;gt;

&lt;span class="c"&gt;# Take External snapshot&lt;/span&gt;
virsh snapshot-create-as --domain &amp;lt;DOMAIN&amp;gt; &amp;lt;SNAPNAME&amp;gt;  &lt;span class="se"&gt;\&lt;/span&gt;
    --diskspec vda,file&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;pool_dir&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/snaps/&amp;lt;SNAPNAME&amp;gt;.qcow2 &lt;span class="se"&gt;\&lt;/span&gt;
    --disk-only --atomic --no-metadata

&lt;span class="c"&gt;# Run Geo-replication&lt;/span&gt;
/usr/share/glusterfs/scripts/schedule_georep.py &lt;span class="se"&gt;\&lt;/span&gt;
    &amp;lt;MASTERVOL&amp;gt; &amp;lt;SLAVE&amp;gt; &amp;lt;SLAVEVOL&amp;gt;

&lt;span class="c"&gt;# Blockcommit&lt;/span&gt;
virsh blockcommit &amp;lt;DOMAIN&amp;gt; vda --active --verbose --pivot

&lt;span class="c"&gt;# Remove the external Snapshot file&lt;/span&gt;
rm &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;pool_dir&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/snaps/&amp;lt;SNAPNAME&amp;gt;.qcow2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With this method, Slave will always have consitent base image.&lt;/p&gt;
&lt;p&gt;Ref: &lt;a class="reference external" href="http://wiki.libvirt.org/page/Live-disk-backup-with-active-blockcommit"&gt;http://wiki.libvirt.org/page/Live-disk-backup-with-active-blockcommit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We should use qcow2 external snapshot if Live backup is
required. External snapshot file will be deleted once blockcommit is
done in Master side.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="gluster"></category><category term="glusterfsblog"></category></entry><entry><title>Interfaces for Gluster Management</title><link href="http://aravindavk.in/blog/interfaces-for-gluster-management/" rel="alternate"></link><updated>2016-02-09T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2016-02-09:blog/interfaces-for-gluster-management/</id><summary type="html">&lt;p&gt;Gluster provides CLIs to manage the Cluster, which can be
programmatly consumed by passing &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--xml&lt;/span&gt;&lt;/tt&gt; option. For example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;gluster volume info --xml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But Gluster CLIs are not enough for managing from remote place or to
integrate with third party Management/Monitoring applications like &lt;a class="reference external" href="http://cockpit-project.org/"&gt;Cockpit&lt;/a&gt;,
&lt;a class="reference external" href="https://github.com/skyrings/skyring"&gt;Skyring&lt;/a&gt;, &lt;a class="reference external" href="http://nagios.org/"&gt;Nagios&lt;/a&gt; etc. We need more interfaces to enable integration
with these tools.&lt;/p&gt;
&lt;div class="section" id="language-bindings-for-gluster-cli-commands"&gt;
&lt;h2&gt;Language bindings for Gluster CLI commands&lt;/h2&gt;
&lt;p&gt;How about importing the &lt;tt class="docutils literal"&gt;glustercli&lt;/tt&gt; in your favorite programming
language and start using it?&lt;/p&gt;
&lt;p&gt;For example in Python,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glustercli&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;

&lt;span class="n"&gt;gv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Volume&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gv1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; &lt;a class="reference external" href="https://github.com/aravindavk/glustertool/tree/master/glustertool/utils/glustercli"&gt;Python&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/aravindavk/glustercli"&gt;Go&lt;/a&gt; bindings are in progress&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ansible-apis-for-gluster"&gt;
&lt;h2&gt;Ansible APIs for Gluster&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.ansible.com/"&gt;Ansible&lt;/a&gt; is present favorite tool for Sysadmins.&lt;/p&gt;
&lt;blockquote&gt;
App deployment, configuration management and orchestration - all from one system. - www.ansible.com&lt;/blockquote&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/gluster/gdeploy"&gt;gdeploy&lt;/a&gt; (name may change in future) is a Ansible based tool for easy management of Gluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; &lt;a class="reference external" href="https://github.com/gluster/gdeploy/blob/2.0/doc/gdeploy-2"&gt;gdeploy&lt;/a&gt; team is working on version 2.0&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="storaged-gluster-apis"&gt;
&lt;h2&gt;Storaged Gluster APIs&lt;/h2&gt;
&lt;p&gt;Gluster module for &lt;a class="reference external" href="http://storaged-project.github.io/"&gt;storaged&lt;/a&gt; enables integration with Cockpit. Cockpit
can communicate with Storaged to get Gluster tasks done.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; &lt;a class="reference external" href="https://samxan.wordpress.com/"&gt;Samikshan&lt;/a&gt; is working on this feature.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="rest-apis-for-gluster"&gt;
&lt;h2&gt;REST APIs for Gluster&lt;/h2&gt;
&lt;p&gt;Common API format to integrate with Web applications. Any
web application can easily communicate with Gluster using HTTP calls.&lt;/p&gt;
&lt;p&gt;For example, Web application can send HTTP POST request to Start the
Volume&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;curl -X POST &lt;span class="se"&gt;\&lt;/span&gt;
    -H &lt;span class="s2"&gt;&amp;quot;content-type: application/json&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    -H &lt;span class="s2"&gt;&amp;quot;Date: Tue, 09 Feb 2016 12:38:10 +0000&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    -H &lt;span class="s2"&gt;&amp;quot;Authorization: HMAC_SHA256 MyApp:g0b1IOmdRMOlPs2f5D4UJPgng9tNUuY0k+c+ee/k2Hk=&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    http://hostname/v1/volumes/gv1/start
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; In progress. Watch this space for more update about this feature :)&lt;/p&gt;
&lt;p&gt;Are you using any interface for managing Gluster? Please share your
experiences.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="gluster"></category><category term="glusterfsblog"></category></entry><entry><title>Simulating Race Conditions</title><link href="http://aravindavk.in/blog/simulating-race-conditions/" rel="alternate"></link><updated>2015-09-11T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2015-09-11:blog/simulating-race-conditions/</id><summary type="html">&lt;p&gt;&lt;a class="reference external" href="http://gluster.readthedocs.org/en/release-3.7.0/Features/tier/"&gt;Tiering&lt;/a&gt; feature is introduced in &lt;a class="reference external" href="http://www.gluster.org/"&gt;Gluster 3.7&lt;/a&gt; release. Geo-replication may not perform well with Tiering feature yet. Races can happen since Rebalance moves files from one brick to another brick(hot to cold and cold to hot), but the Changelog/Journal remails in old brick itself. We know there will be problems since each Geo-replication worker(per brick) processes Changelogs belonging to respective brick and sync the data independently. Sync happens as two step operation, Create entry in Slave with the GFID recorded in Changelog, then use Rsync to sync data(using GFID access)&lt;/p&gt;
&lt;p&gt;To uncover the bugs we need to setup workload and run multiple times since issues may not happen always. But it is tedious to run multiple times with actual data. How about simulating/mocking it?&lt;/p&gt;
&lt;p&gt;Let us consider simple case of Rebalance, A file &amp;quot;f1&amp;quot; is created in Brick1 and after some time it becomes hot and Rebalance moved it to Brick2.&lt;/p&gt;
&lt;img alt="Rebalance explained" src="/images/rebalance.png" /&gt;
&lt;p&gt;In Changelog we don't capture the Rebalance Traffic, so in respective brick changelogs will contain,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# Brick1 Changelog
CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Brick2 Changelog
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If Brick1 worker processes fast, then Entry is created in Slave and Data Operation succeeds. Since Both the workers can independently, sequence of execution may be like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# Possible Sequence 1
[Brick1] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick1] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
[Brick2] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Possible Sequence 2
[Brick2] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
[Brick1] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick1] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Possible Sequence 3
[Brick1] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick2] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
[Brick1] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We don't have any problems with first and last sequence, But in second sequence Rsync will try to sync data before Entry Creation and Fails.&lt;/p&gt;
&lt;p&gt;To solve this issue, we thought if we record CREATE from Rebalance traffic then it will solve this problem. So now brick Changelogs looks like,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# Brick1 Changelog
CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Brick2 Changelog
CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and possible sequences,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# Possible Sequence 1
[Brick1] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick1] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
[Brick2] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick2] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Possible Sequence 2
[Brick2] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick1] CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
[Brick1] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
[Brick2] DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# and many more...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We do not have that problem, second CREATE will fail with EEXIST, we ignore it since it is safe error. But will this approach solves all the problems with Rebalance? When more FOPs added, it is very difficult to visualize or guess the problem.&lt;/p&gt;
&lt;p&gt;To mock the concurrent workload, Collect sequence from each bricks Changelog and mix both the sequences. We should make sure that order in each brick remains same after the mix.&lt;/p&gt;
&lt;p&gt;For example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;b1 = [&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;]
b2 = [&amp;quot;F&amp;quot;, &amp;quot;G&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While mixing b2 in b1, for first element in b2 we can randomly choose a position in b1. Let us say random position we got is 2(Index is 2), and insert &amp;quot;F&amp;quot; in index 2 of b1&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# before
[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;]
# after
[&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, to insert &amp;quot;G&amp;quot;, we should randomly choose anywhere after &amp;quot;F&amp;quot;. Once we get the sequence, mock the FOPs and compare with expected values.&lt;/p&gt;
&lt;p&gt;I added a &lt;a class="reference external" href="https://gist.github.com/aravindavk/193eda60b6049ad025f4"&gt;gist&lt;/a&gt; for testing following workload, it generates multiple sequences for testing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# f1 created in Brick1, Rebalanced to Brick2 and then Unlinked
# Brick1 Changelog
CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef

# Brick2 Changelog
CREATE 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
DATA 0945daec-6f8c-438e-9bbf-b2ebf07543ef
UNLINK 0945daec-6f8c-438e-9bbf-b2ebf07543ef f1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Found two bugs.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Trying to sync data after UNLINK(Which can be handled in Geo-rep by Rsync retry)&lt;/li&gt;
&lt;li&gt;Empty file gets created.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I just started simulating with Tiering + Geo-replication workload, I may encounter more problems with Renames(Simple, multiple and cyclic). Will update the results soon.&lt;/p&gt;
&lt;p&gt;I am sharing the script since it can be easily modified to work with different workloads and to test other projects/components.&lt;/p&gt;
&lt;p&gt;Let me know if this is useful. Comments and Suggestions Welcome.&lt;/p&gt;
</summary><category term="gluster"></category><category term="glusterfsblog"></category></entry><entry><title>Introducing georepsetup - Gluster Geo-replication Setup Tool</title><link href="http://aravindavk.in/blog/introducing-georepsetup/" rel="alternate"></link><updated>2015-09-02T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2015-09-02:blog/introducing-georepsetup/</id><summary type="html">&lt;p&gt;How many of you succeeded to set up Gluster Geo-replication for the first time? SSH keys need to be deployed to all Slave nodes from all Master nodes as part of the Geo-replication setup. So number of steps involved in setting up Geo-rep is not very easy to manage. We get more queries in &lt;a class="reference external" href="http://www.gluster.org/mailman/listinfo/gluster-devel"&gt;gluster-devel&lt;/a&gt; and &lt;a class="reference external" href="http://www.gluster.org/mailman/listinfo/gluster-users"&gt;gluster-users&lt;/a&gt; lists related to Geo-rep Setup than actually using Geo-replication, many users stopped trying Geo-replication after they faced issues during setup.&lt;/p&gt;
&lt;p&gt;With the release of Gluster 3.7, the Geo-replication got lots of improvements. Will write blog about new features and improvements in my next blog. Yesterday I wrote a CLI tool using Python to simplify the steps involved in Geo-replication setup. Now setting up Geo-replication is as easy as running one command. Yay!&lt;/p&gt;
&lt;p&gt;For example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo georepsetup &amp;lt;MASTERVOL&amp;gt; &amp;lt;SLAVEHOST&amp;gt; &amp;lt;SLAVEVOL&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It prompts for the Root's Password of Slave node specified in the command. That's it!&lt;/p&gt;
&lt;p&gt;This command also produces a good summary as shown below. Now it is very easy to trace the errors and handle them.&lt;/p&gt;
&lt;img alt="Summary" src="/images/georepsetup.png" /&gt;
&lt;div class="section" id="install"&gt;
&lt;h2&gt;Install&lt;/h2&gt;
&lt;p&gt;Install this tool on any one master node where you wish to initiate the Geo-replication setup,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;git clone https://github.com/aravindavk/georepsetup.git
&lt;span class="nb"&gt;cd &lt;/span&gt;georepsetup
sudo python setup.py install
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This tool is not packaged as RPM/Deb Yet. Pull requests are Welcome :)&lt;/p&gt;
&lt;p&gt;Setting up non-root Geo-replication still involves some manual steps, will try to improve in future.&lt;/p&gt;
&lt;p&gt;Documentation is available &lt;a class="reference external" href="https://github.com/aravindavk/georepsetup/blob/master/README.md"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments &amp;amp; Suggestions Welcome.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="geo-replication"></category><category term="gluster"></category><category term="glusterfsblog"></category></entry><entry><title>GlusterFS Geo-replication Tutorials - Understanding Session Creation</title><link href="http://aravindavk.in/blog/glusterfs-georeplication-tutorials-1/" rel="alternate"></link><updated>2015-04-02T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2015-04-02:blog/glusterfs-georeplication-tutorials-1/</id><summary type="html">&lt;p&gt;Geo-replication is one of the awesome feature of &lt;a class="reference external" href="http://gluster.org/"&gt;GlusterFS&lt;/a&gt;. With this feature we can replicate data from one Gluster Volume to another geographically located Gluster Volume.&lt;/p&gt;
&lt;p&gt;This blog is first in a series of Understanding GlusterFS Geo-replication, Comments and Suggestions welcome.&lt;/p&gt;
&lt;script async class="speakerdeck-embed" data-id="f509ae7c9216494fa690f8dfee0e91c1" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"&gt;
&lt;/script&gt;&lt;p&gt;Link: &lt;a class="reference external" href="https://speakerdeck.com/aravindavk/understanding-glusterfs-geo-replication-session-creation"&gt;https://speakerdeck.com/aravindavk/understanding-glusterfs-geo-replication-session-creation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I created these visualizations using my Wacom tablet(Wacom Bamboo Pen &amp;amp; Touch CTH-460) and &lt;a class="reference external" href="http://mypaint.intilinux.com/"&gt;MyPaint&lt;/a&gt; software in Linux.&lt;/p&gt;
</summary><category term="geo-replication"></category><category term="glusterfs"></category><category term="visualizations"></category><category term="glusterfsblog"></category></entry><entry><title>Introducing gdash - GlusterFS Dashboard</title><link href="http://aravindavk.in/blog/introducing-gdash/" rel="alternate"></link><updated>2014-12-04T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2014-12-04:blog/introducing-gdash/</id><summary type="html">&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; Added &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--gluster&lt;/span&gt;&lt;/tt&gt; option to specify the path to gluster. By default it looks for &lt;tt class="docutils literal"&gt;/usr/sbin/gluster&lt;/tt&gt;, If you installed GlusterFS using source install then use &lt;code&gt;sudo gdash --gluster /usr/local/sbin/gluster&lt;/code&gt;. (Those who already installed gdash, can run &lt;code&gt;sudo pip install -U gdash&lt;/code&gt; to upgrade.)&lt;/p&gt;
&lt;p&gt;gdash is a super-young project, which shows GlusterFS volume information about local, remote clusters. This app is based on GlusterFS's capability of executing &lt;code&gt;gluster volume info&lt;/code&gt; and &lt;code&gt;gluster volume status&lt;/code&gt; commands for a remote server using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--remote-host&lt;/span&gt;&lt;/tt&gt; option.&lt;/p&gt;
&lt;p&gt;If you can run &lt;code&gt;gluster volume info --remote-host=&amp;lt;HOST_NAME&amp;gt;&lt;/code&gt;, then you can monitor that cluster using gdash. Make sure you allow to access glusterd port(24007) for the machine where you will run gdash.&lt;/p&gt;
&lt;p&gt;To install,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo pip install gdash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo easy_install gdash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;gdash is created using Python &lt;a class="reference external" href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; and &lt;a class="reference external" href="http://emberjs.com/"&gt;ember&lt;/a&gt; (I used &lt;a class="reference external" href="http://ember-cli.com"&gt;ember-cli&lt;/a&gt;).&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="gdash home screen" src="/images/gdash-home.png" /&gt;
&lt;p class="caption"&gt;gdash home screen&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="gdash detail screen" src="/images/gdash-detail.png" /&gt;
&lt;p class="caption"&gt;gdash Volume details page&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="usage"&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;div class="section" id="use-case-1-local-volumes"&gt;
&lt;h3&gt;Use case 1 - Local Volumes&lt;/h3&gt;
&lt;p&gt;Just run &lt;code&gt;sudo gdash&lt;/code&gt;, gdash starts running in port 8080. visit &lt;a class="reference external" href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; to view GlusterFS volumes of local machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="use-case-2-remote-volumes"&gt;
&lt;h3&gt;Use case 2 - Remote Volumes&lt;/h3&gt;
&lt;p&gt;Run &lt;code&gt;sudo gdash --host 192.168.1.6&lt;/code&gt;, visit &lt;a class="reference external" href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; to view GlusterFS volume information of remote host. Dashboard shows all the volumes which are part of that remote host.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="use-case-3-multiple-clusters"&gt;
&lt;h3&gt;Use case 3 - Multiple clusters&lt;/h3&gt;
&lt;p&gt;Create a clusters.conf file as example shown below, specify at least one host from each cluster.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[clusters]&lt;/span&gt;
&lt;span class="na"&gt;cluster1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;host1, host2, host3&lt;/span&gt;
&lt;span class="na"&gt;cluster2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;host4, host5, host6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Run &lt;code&gt;gdash&lt;/code&gt; using,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gdash --clusters ~/clusters.conf
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="use-case-4-multiple-teams"&gt;
&lt;h3&gt;Use case 4 - Multiple teams&lt;/h3&gt;
&lt;p&gt;If two teams monitoring two clusters and if you don't want to share the other cluster details then, just run below commands in two terminals and give respective URL to each team. Other solution is create two seperate config files and run it separately for different ports.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Team 1, who monitors cluster1 http://localhost:8001&lt;/span&gt;
sudo gdash -p &lt;span class="m"&gt;8001&lt;/span&gt; --clusters ~/clusters.conf --limit-cluster cluster1

&lt;span class="c"&gt;# Team 2, who monitors cluster2 http://localhost:8002&lt;/span&gt;
sudo gdash -p &lt;span class="m"&gt;8002&lt;/span&gt; --clusters ~/clusters.conf --limit-cluster cluster2
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="available-options"&gt;
&lt;h2&gt;Available Options&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;usage: gdash [-h] [--port PORT] [--cache CACHE] [--debug] [--host HOST]
             [--clusters CLUSTERS] [--limit-cluster LIMIT_CLUSTER]

GlusterFS dashboard
-------------------

This tool is based on remote execution support provided by
GlusterFS cli for `volume info` and `volume status` commands

optional arguments:
  -h, --help            show this help message and exit
  --port PORT, -p PORT  Port
  --cache CACHE, -c CACHE
                        Cache output in seconds
  --debug               DEBUG
  --host HOST           Remote host which is part of cluster
  --clusters CLUSTERS   Clusters CONF file
  --limit-cluster LIMIT_CLUSTER
                        Limit dashboard only for specified cluster
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Code is hosted in &lt;a class="reference external" href="https://github.com/aravindavk/gdash"&gt;github/aravindavk&lt;/a&gt;, licensed under &lt;a class="reference external" href="https://github.com/aravindavk/gdash/blob/master/LICENSE.txt"&gt;MIT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="glusterfs"></category><category term="tools"></category><category term="glusterfsblog"></category></entry><entry><title>gvolinfojson - A utility to convert xml output of gluster volume info to json</title><link href="http://aravindavk.in/blog/gvolinfojson/" rel="alternate"></link><updated>2014-05-13T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2014-05-13:blog/gvolinfojson/</id><summary type="html">&lt;p&gt;Today I wrote a small utility using &lt;a class="reference external" href="http://golang.org/"&gt;golang&lt;/a&gt; to convert xml output of command &lt;code&gt;gluster volume info&lt;/code&gt; to json.&lt;/p&gt;
&lt;p&gt;Download the binary from &lt;a class="reference external" href="https://github.com/aravindavk/gvolinfojson/releases/download/1.0/gvolinfojson"&gt;here&lt;/a&gt; and copy to /usr/local/bin directory(or any other directory, which is available in PATH).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;wget https://github.com/aravindavk/gvolinfojson/releases/download/1.0/gvolinfojson
sudo cp gvolinfojson /usr/local/bin/
sudo chmod +x /usr/local/bin/gvolinfojson
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;p&gt;If you have golang installed(make sure &lt;code&gt;$GOPATH/bin&lt;/code&gt; is available in PATH), then&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;go get github.com/aravindavk/gvolinfojson
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use it with gluster volume info command,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gluster volume info --xml | gvolinfojson
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thats it, you will get the json output of volume info command. If you need pretty json output then&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gluster volume info --xml | gvolinfojson --pretty
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Source code is available &lt;a class="reference external" href="https://github.com/aravindavk/gvolinfojson"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;C &amp;amp; S Welcome.&lt;/p&gt;
</summary><category term="glusterfs"></category><category term="tools"></category><category term="glusterfsblog"></category></entry><entry><title>Effective GlusterFs monitoring using hooks</title><link href="http://aravindavk.in/blog/effective-glusterfs-monitoring-using-hooks/" rel="alternate"></link><updated>2013-11-28T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2013-11-28:blog/effective-glusterfs-monitoring-using-hooks/</id><summary type="html">&lt;p&gt;Let us imagine we have a GlusterFs monitoring system which displays list of volumes and its state, to show the realtime status, monitoring app need to query the GlusterFs in regular interval to check volume status, new volumes etc. Assume if the polling interval is 5 seconds then monitoring app has to run &lt;code&gt;gluster volume info&lt;/code&gt; command ~17000 times a day!&lt;/p&gt;
&lt;p&gt;How about maintaining a state file in each node? which gets updated after every new GlusterFs event(create, delete, start, stop etc).&lt;/p&gt;
&lt;p&gt;In this blog post I am trying to explain the possibility of creating state file and using it.&lt;/p&gt;
&lt;p&gt;As of today GlusterFs provides following hooks, which we can use to update our state file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;create
delete
start
stop
add-brick
remove-brick
set
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="how-to-use-hooks"&gt;
&lt;h2&gt;How to use hooks&lt;/h2&gt;
&lt;p&gt;GlusterFs hooks present in &lt;code&gt;/var/lib/glusterd/hooks/1&lt;/code&gt; directory. Following example shows sending message to all users using &lt;code&gt;wall&lt;/code&gt; command when any new GlusterFs volume is created.&lt;/p&gt;
&lt;p&gt;Create a shell script &lt;code&gt;/var/lib/glusterd/hooks/1/create/post/SNotify.bash&lt;/code&gt; and make it executable. Whenever a volume is created GlusterFs executes all the executable scripts present in respective hook directory(Glusterfs executes only the scripts which filename starting with 'S')&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nv"&gt;VOL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="nv"&gt;ARGS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;getopt -l &lt;span class="s2"&gt;&amp;quot;volname:&amp;quot;&lt;/span&gt;  -name &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;eval set&lt;/span&gt; -- &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$ARGS&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt; in
        --volname&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="nb"&gt;shift&lt;/span&gt;
&lt;span class="nb"&gt;            &lt;/span&gt;&lt;span class="nv"&gt;VOL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;
            &lt;span class="p"&gt;;;&lt;/span&gt;
        *&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="nb"&gt;shift&lt;/span&gt;
&lt;span class="nb"&gt;            break&lt;/span&gt;
            &lt;span class="p"&gt;;;&lt;/span&gt;
    &lt;span class="k"&gt;esac&lt;/span&gt;
    &lt;span class="nb"&gt;shift&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;

wall &lt;span class="s2"&gt;&amp;quot;Gluster Volume Created: &lt;/span&gt;&lt;span class="nv"&gt;$VOL&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="experimental-project-glusterweb"&gt;
&lt;h2&gt;Experimental project - GlusterWeb&lt;/h2&gt;
&lt;p&gt;This experimental project maintains a sqlite database &lt;code&gt;/var/lib/glusterd/nodestate/glusternodestate.db&lt;/code&gt; which gets updated after any GlusterFs event. For example if a GlusterFs volume is created then it updates volumes table and also bricks table.&lt;/p&gt;
&lt;p&gt;This project depends on &lt;a class="reference external" href="https://github.com/aravindavk/glusterfs-tools"&gt;glusterfs-tools&lt;/a&gt; so install both projects.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;git clone https://github.com/aravindavk/glusterfs-tools.git
&lt;span class="nb"&gt;cd &lt;/span&gt;glusterfs-tools
sudo python setup.py install

git clone https://github.com/aravindavk/glusterfs-web.git
&lt;span class="nb"&gt;cd &lt;/span&gt;glusterfs-web
sudo python setup.py install
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By running &lt;cite&gt;setup&lt;/cite&gt;, this tool will install all the hooks which are required for monitoring. (&lt;cite&gt;cleanup&lt;/cite&gt; is for removing all the hooks)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo glusternodestate setup
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All set! now run &lt;code&gt;glusterweb&lt;/code&gt; to start webapp.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo glusterweb
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Web application starts running in &lt;code&gt;http://localhost:8080&lt;/code&gt; you can change the port using &lt;code&gt;--port&lt;/code&gt; or &lt;code&gt;-p&lt;/code&gt; option.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo glusterweb -p 9000
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="GlusterWeb" src="/images/glusterweb-v0.1.png" /&gt;
&lt;p class="caption"&gt;Initial version of web interface.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="future-plans"&gt;
&lt;h2&gt;Future plans&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: Option to provide username and password or access key while running glusterweb, For example&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo glusterweb --username aravindavk --password somesecret
&lt;span class="c"&gt;# or&lt;/span&gt;
sudo glusterweb --key secretonlyiknow
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;More gluster hooks support:&lt;/strong&gt; we need more GlusterFs hooks for better monitoring(refer Problems below)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More GlusterFs features support:&lt;/strong&gt; As a experiment UI only lists volumes, we need improved UI and support for different gluster features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Actions support:&lt;/strong&gt; Support for volume creation, adding/removing bricks etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;REST api and SDK:&lt;/strong&gt; Providing REST api for gluster operations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Many more:&lt;/strong&gt; Not yet planned :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="problems"&gt;
&lt;h2&gt;Problems&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;State file consistency:&lt;/strong&gt; If glusterd goes down in the node then the database will have wrong details about node's state. One workaround is to reset the database if glusterd is down using a cron job, when glusterd comes up, database will not gets updated and the database will have previous updated details. To prevent this we need a glusterfs hook for &lt;cite&gt;glusterd-start&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;More hooks:&lt;/strong&gt; As of today we don't have hooks for volume down/up, brick down/up and other events. We need following hooks for effective monitoring glusterfs.(Add more if anything missing in the list)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;glusterd-start
peer probe
peer detach
volume-down
volume-up
brick-up
brick-down
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let me know your thoughts! Thanks.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="glusterfs"></category><category term="glusterfsblog"></category></entry><entry><title>glusterdf - df for gluster volumes</title><link href="http://aravindavk.in/blog/glusterdf-df-for-gluster-volumes/" rel="alternate"></link><updated>2013-09-24T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2013-09-24:blog/glusterdf-df-for-gluster-volumes/</id><summary type="html">&lt;p&gt;A CLI utility to check the disk usage of &lt;a class="reference external" href="http://gluster.org/"&gt;glusterfs&lt;/a&gt; volumes. Using &lt;code&gt;df&lt;/code&gt; command we can view the disk usage of only mounted glusterfs volumes. This utility takes care of mounting gluster volumes available in the machine where this command is executed. glusterdf uses &lt;a class="reference external" href="https://github.com/gluster/glusterfs/tree/master/api"&gt;libgfapi&lt;/a&gt; provided by glusterfs to fetch the statvfs information.&lt;/p&gt;
&lt;p&gt;Installation is very simple,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;git clone https://github.com/aravindavk/glusterfs-tools.git
&lt;span class="nb"&gt;cd &lt;/span&gt;glusterfs-tools
sudo python setup.py install
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also clone this project from &lt;a class="reference external" href="https://forge.gluster.org/glusterfs-tools"&gt;forge.gluster.org/glusterfs-tools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once installed, two tools will be available &lt;code&gt;glustervolumes&lt;/code&gt; and &lt;cite&gt;glusterdf&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo glusterdf --help&lt;/code&gt; to know more about options available. (same for glustervolumes &lt;cite&gt;sudo glustervolumes --help&lt;/cite&gt;)&lt;/p&gt;
&lt;p&gt;Usage examples:&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="glusterdf -h" src="/images/glusterfs/glusterdf_h.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf -h (Disk usage in human readable format)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="glusterdf -i" src="/images/glusterfs/glusterdf_i.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf -i (View inodes usage information)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="sudo glusterdf --status up --type repl -h" src="/images/glusterfs/glusterdf_status_type_h.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf --status up --type repl -h (View all running replicated volumes)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="sudo glusterdf -h --volumewithbrick &amp;quot;/b[12]&amp;quot;" src="/images/glusterfs/glusterdf_volumewithbrick.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf -h --volumewithbrick &amp;quot;/b[12]&amp;quot;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="sudo glusterdf --status up --type repli -h --json | python -m json.tool" src="/images/glusterfs/glusterdf_json.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf --status up --type repli -h --json | python -m json.tool&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="glusterdf --help" src="/images/glusterfs/glusterdf-help.png" /&gt;
&lt;p class="caption"&gt;sudo glusterdf --help&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In my previous blog(&lt;a class="reference external" href="http://aravindavk.in/blog/glusterfs-tools/"&gt;this&lt;/a&gt;) I wrote about gfvolumes(now it is &lt;cite&gt;glustervolumes&lt;/cite&gt;). glusterfs-tools is rewritten as python library which can be used with your Python programs.&lt;/p&gt;
&lt;p&gt;For example&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glusterfstools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gfapi&lt;/span&gt;
&lt;span class="c"&gt;# Get all volumes&lt;/span&gt;
&lt;span class="n"&gt;vols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;# Get a specific volume information&lt;/span&gt;
&lt;span class="n"&gt;vol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gv1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Search volumes by status&lt;/span&gt;
&lt;span class="n"&gt;down_volumes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;down&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="c"&gt;# Search volumes by type&lt;/span&gt;
&lt;span class="n"&gt;distribute_volumes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;distribute&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="c"&gt;# Statvfs information&lt;/span&gt;
&lt;span class="n"&gt;vol_statvfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gfapi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;statvfs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gv1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# To view information about gluster volumes which are down&lt;/span&gt;
&lt;span class="c"&gt;# and having bricks like &amp;quot;/gfs&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;vols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;down&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;volumewithbricks&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/gfs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="c"&gt;# To view filters available&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;volumes.search accepts filters as parameter, extending volume filters is very simple. For example name filter looks like this(&lt;a class="reference external" href="https://github.com/aravindavk/glusterfs-tools/blob/master/src/glusterfstools/volumefilters.py"&gt;src/glusterfstools/volumefilters.py&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nd"&gt;@filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;name_filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vols&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;is_match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; \
            &lt;span class="n"&gt;vol&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; \
            &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vol&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vols&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;is_match&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The filter can be used as below&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;glusterfstools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;

&lt;span class="c"&gt;# Filters the volumes with name either gv1 or gv2&lt;/span&gt;
&lt;span class="n"&gt;filters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gv[12]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</summary><category term="glusterfs"></category><category term="tools"></category><category term="glusterfsblog"></category></entry><entry><title>GlusterFS Tools</title><link href="http://aravindavk.in/blog/glusterfs-tools/" rel="alternate"></link><updated>2013-06-18T00:00:00+05:30</updated><author><name>Aravinda VK</name></author><id>tag:aravindavk.in,2013-06-18:blog/glusterfs-tools/</id><summary type="html">&lt;div class="notice-update"&gt;
UPDATE: &lt;br/&gt;Installation and usage is simplified with the new release of glusterfs-tools, refer &lt;a href="http://aravindavk.in/blog/glusterdf-df-for-gluster-volumes/"&gt;this blog&lt;/a&gt; for more details.
&lt;/div&gt;&lt;p&gt;From &lt;a class="reference external" href="http://gluster.org"&gt;GlusterFS website&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
GlusterFS is an open source, distributed file system capable of scaling to several petabytes (actually, 72 brontobytes!) and handling thousands of clients. GlusterFS clusters together storage building blocks over Infiniband RDMA or TCP/IP interconnect, aggregating disk and memory resources and managing data in a single global namespace. GlusterFS is based on a stackable user space design and can deliver exceptional performance for diverse workloads.&lt;/blockquote&gt;
&lt;p&gt;Gluster CLI has limited features to view and filter the volume info. I started a small project to enhance Gluster CLI for personal use. As of now it consists of a tool to list Gluster volumes in tabular format. Other intersesting features includes filtering the output based on name, type, status, bricks etc.&lt;/p&gt;
&lt;p&gt;Clone the project(I cloned it to &lt;code&gt;/home/aravinda/sandbox/&lt;/code&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; /home/aravinda/sandbox
git clone https://github.com/aravindavk/glusterfs-tools.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create a shellscript to call gftools /usr/local/bin/gfvolumes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
python /home/aravinda/sandbox/glusterfs-tools/gftools/volumes.py &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make gfvolumes executable&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;chmod +x /usr/local/bin/gfvolumes
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can run &lt;code&gt;sudo gfvolumes&lt;/code&gt; to see the list of glusterfs volumes. Type &lt;code&gt;gfvolumes --help&lt;/code&gt; for help.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="All Volumes" src="/images/glusterfs/all_volumes.png" /&gt;
&lt;p class="caption"&gt;All Volumes&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="Name Filter" src="/images/glusterfs/name_filter.png" /&gt;
&lt;p class="caption"&gt;Name Filter&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="Status Filter" src="/images/glusterfs/status_filter.png" /&gt;
&lt;p class="caption"&gt;Status Filter&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="Type Filter" src="/images/glusterfs/type_filter.png" /&gt;
&lt;p class="caption"&gt;Type Filter&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;img alt="Name Filter" src="/images/glusterfs/show_bricks.png" /&gt;
&lt;p class="caption"&gt;Show Bricks&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Additionally it can output filtered details in JSON format.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;img alt="Name Filter" src="/images/glusterfs/json_format.png" /&gt;
&lt;p class="caption"&gt;JSON Format&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can easily import this in our python script.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/python&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gftools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;
&lt;span class="n"&gt;gfvols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;volumes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GfVolumes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ok&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gfvols&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;^gv[0-9]$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;down&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# Various filters available&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;ok&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Do action&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; root permission is required to run gluster command, so run gfvolumes as root(&lt;code&gt;sudo gfvolumes&lt;/code&gt;)&lt;/p&gt;
&lt;div class="section" id="future-plans"&gt;
&lt;h2&gt;Future plans:&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Adding more filters&lt;/li&gt;
&lt;li&gt;Adding more admin tools&lt;/li&gt;
&lt;li&gt;Creating RPM/DEB packages&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;C &amp;amp; S Welcome.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="glusterfs"></category><category term="tools"></category><category term="glusterfsblog"></category></entry></feed>